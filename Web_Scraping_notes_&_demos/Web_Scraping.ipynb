{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49a30a2a-d8f6-48dc-9ea8-649fd493839a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11898e5-0b7c-4419-8114-8712354fe9f2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1c5b3-e260-486a-9c84-2a35d8e11ffa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9243734d-8edd-4c4b-bfdd-20556da27714",
   "metadata": {},
   "source": [
    "# What is Web Scraping?\n",
    "- Web scraping is the automated process of gathering data from websites.\n",
    "- It's like a bot that navigates through a webpage and collects data based on predefined instructions.\n",
    "- This data can range from product prices to text content on articles, images, or structured data in tables.<br>\n",
    "<img src=\"WebScraping_img/Web_Scraping.jpeg\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bfa274-b00d-400f-b0b8-8ee1f78e28e3",
   "metadata": {},
   "source": [
    "### How is Web Scraping different from Web Crawling?\n",
    "- Web crawling, also known as spidering, is the process of systematically navigating the internet to discover and index web pages.\n",
    "- Web crawlers (or spiders) start from a set of URLs, visit each page, extract links to other pages, and continue visiting new pages in a recursive\n",
    "manner.\n",
    "- This enables the crawler to build an extensive index of web pages across a domain or even the entire internet.\n",
    "- The main goal of web crawling is to find and catalog all accessible pages on the web.\n",
    "- Crawled pages are often stored in a database or index for later retrieval and use, such as by search engines or content aggregation tools.\n",
    "- Web Scraping targets particular data points within a webpage, such as prices, reviews, product listings, or other structured information.\n",
    "- Scraping is focused on extracting certain elements or fields from a webpage, rather than exploring links or indexing the entire page.\n",
    "<img src=\"WebScraping_img/2.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d61c371-624b-49c8-a2c1-5c8875289bfb",
   "metadata": {},
   "source": [
    "### How does Web Scraping work?\n",
    "1. HTTP Requests:\n",
    "    - Web scrapers initiate HTTPS requests to servers to retrieve the HTML source of a webpage.\n",
    "    - The GET and POST are most commonly used request types.\n",
    "2. Parsing HTML:\n",
    "    - The script navigates through the received HTML structure to identify and extract data of interest.\n",
    "    - This involves extracting only the required specific data.\n",
    "3. Storage:\n",
    "    - After extraction, data is cleaned and stored in the desired format.\n",
    "    - Data is usually stored in a database, CSV file, or spreadsheet for further analysis.\n",
    "<img src=\"WebScraping_img/4.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af166078-a26b-4169-b115-b460f37e3384",
   "metadata": {},
   "source": [
    "### Types of Web Scraping\n",
    "1. HTML Parsing:\n",
    "    - HTML parsing is the most common form of web scraping.\n",
    "    - It involves analyzing a web page’s HTML structure to extract relevant data.\n",
    "    - Works well for websites with static content or basic HTML structures.\n",
    "    - Example: Extracting blog titles, author names, and publication dates from a blog page.\n",
    "2. Data Object Model (DOM) Parsing:\n",
    "    - Focuses on navigating the DOM structure of a website.\n",
    "    - The DOM structure refers to the hierarchy of elements of the webpage.\n",
    "    - Works best with complex or dynamic websites where content might change upon certain events, such as clicking or scrolling.\n",
    "<img src=\"WebScraping_img/HTML_vs_DOM.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "<img src=\"WebScraping_img/dom-tree.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "3. Headless Browser Scraping:\n",
    "    - Headless browser scraping involves using a browser in headless mode to render web pages like a real user.\n",
    "    - There is no GUI involved in headless browsing. Nothing is display visually on the screen.\n",
    "    - Works best for websites that rely heavily on JavaScript or AJAX to load content.\n",
    "    - Puppeteer is a commonly used tool to work with headless browsers.\n",
    "    - Example: Extracting real-time stock prices from a financial website.\n",
    "4. API-based Scraping:\n",
    "    - Many websites offer APIs (Application Programming Interfaces) for structured data access.\n",
    "    - This can be a more efficient and ethical alternative to traditional scraping methods.\n",
    "    - Example: Extracting user information, posts, and comments from a social media platform’s API.\n",
    "5. Image and Multimedia Scraping:\n",
    "    - Image scraping involves extracting images, videos, or other media files from web pages.\n",
    "    - Scrapers target img tags or other media tags in HTML, and download the files directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f62796-5e82-41e6-931e-a1773d6319cf",
   "metadata": {},
   "source": [
    "### Ethical Consideration\n",
    "- Ethical considerations in web scraping are essential to ensure that data collection practices are conducted\n",
    "responsibly and in line with the legal and moral obligations.\n",
    "- These considerations mainly revolve around respecting website policies, data privacy, intellectual property,\n",
    "and transparency with users.\n",
    "1. Compliance with website Terms & Services:\n",
    "    - Most websites have Terms of Service (ToS) that outline acceptable behaviors, including whether web scraping is permitted\n",
    "    - Violating these terms can result in legal repercussions, as scraping without permission may be viewed as unauthorized access.\n",
    "    - It’s crucial to review and abide by the website’s policies and request explicit permission for data access if the site prohibits scraping.\n",
    "    - **What To Do:** Before starting any scraping activity, read the website’s ToS and Privacy Policy carefully. When in doubt, seek permission or use alternative, sanctioned APIs.\n",
    "2. Respect for Data Ownership and Intellectual Property Rights:\n",
    "    - The data on a website is generally owned by the website’s creators or operators.\n",
    "    - Unauthorized replication or distribution may infringe on intellectual property rights.\n",
    "    - **What To Do:** Use scraped data strictly for purposes that do not violate intellectual property laws and avoid redistributing content without permission.\n",
    "3. Data Privacy and User Consent:\n",
    "    - Websites may contain sensitive or personal information about users, such as names, email addresses, or comments.\n",
    "    - Scraping such data without explicit user consent is a privacy breach.\n",
    "    - Regulations like the GDPR (Europe) and CCPA (USA) impose strict guidelines on handling personal data.\n",
    "    - **What To Do:** Avoid scraping personal data unless you have explicit permission. If personal data is required, ensur compliance with relevant privacy laws.\n",
    "4. Rate Limits and Server Overload:\n",
    "    - Websites operate with limited server resources, and excessive scraping can strain servers, which can slow down performance for other users.\n",
    "    - Ethical scrapers should honor the website’s robots.txt file, which often specifies crawling frequency and areas off-limits to automated access.\n",
    "    - **What To Do:** Implement rate limiting and time intervals between requests to reduce the impact on the website’s server.\n",
    "5. Transparency and Disclosure:\n",
    "    - Ethical web scraping involves transparency about the intent and use of the data, especially if it’s for commercial purposes.\n",
    "    - Using data without context or presenting scraped data as a comprehensive view of a company’s offerings can mislead users and harm the reputation of the data’s original source.\n",
    "    - **What To Do:** If using scraped data for public purposes, clearly disclose its source, the data collection process, and any limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f373a5-2193-4af4-aeae-eeb0161fbf00",
   "metadata": {},
   "source": [
    "### Advantages of Web Scraping\n",
    "1. Efficient Data Collection and Processing:\n",
    "    - Web scraping allows for the automated collection of data at a large scale, offering much higher speed and efficiency than manual collection.\n",
    "    - Helps save considerable time and effort, enabling faster access to information.\n",
    "    - This is particularly beneficial for industries that rely on large datasets, such as e-commerce, market research, and finance.\n",
    "2. Real-Time Data Access:\n",
    "    - Web scraping enables real-time data extraction, allowing companies to monitor data and respond to changes immediately.\n",
    "    - Access to real-time data provides businesses with a competitive edge by allowing them to adjust strategies based on the latest trends.\n",
    "3. Cost-Effective Market Research:\n",
    "    - Compared to traditional data collection methods, such as surveys or purchasing datasets, web scraping offers a cost-effective way to collect market data.\n",
    "    - Web scraping can gather data from various websites, blogs, social media, and online forums, providing a broader view of the market landscape\n",
    "4. Enhanced Decision-Making through Data-Driven Insights:\n",
    "    - Access to data-driven insights enables organizations to make better, evidence-based decisions.\n",
    "    - Web scraping helps compile data that is crucial for understanding consumer behavior, trends, and competitor activities.\n",
    "    - Helps companies analyze historical data to identify trends and predict future behaviors, aiding long-term strategy planning.\n",
    "5. Detecting and Analyzing Fraudulent Activities:\n",
    "    - By monitoring patterns in online data, web scraping can help identify potentially fraudulent activities, such as fake reviews, counterfeit product listings, or misleading advertisements.\n",
    "    - Companies can use web scraping to validate information about their own products and services by comparing data across different platforms, detecting inconsistencies that may indicate fraud\n",
    "6. Enhanced SEO and Content Strategy:\n",
    "    - Web scraping can help companies analyze competitors' keywords, backlinks, and content strategies to improve their own SEO performance.\n",
    "    - Understanding high-performing content on competitors' websites can guide and allow companies to identify and replicate successful topics and formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba8bae-2021-4e9f-9b9b-2c812b1a42c6",
   "metadata": {},
   "source": [
    "### Disadvantages of Web Scraping\n",
    "1. Legal and Ethical Risks:\n",
    "    - Many websites have terms of service that prohibit or limit data scraping.\n",
    "    - Extracting data without permission can lead to copyright issues, potential lawsuits, or restrictions from the website owner.\n",
    "    - Scraping personal information, even if publicly available, can raise privacy issues, especially unde data protection laws like GDPR.\n",
    "    - Companies can face penalties for scraping personal data without consent.\n",
    "2. IP Blocking and Bot Detection:\n",
    "    - Websites often deploy mechanisms like CAPTCHAs, rate limits, and IP blocking to detect and block scraping bots.\n",
    "    - This can interrupt scraping processes, requiring continual adjustment to circumvent these systems.\n",
    "    - Many scrapers use rotating proxies to avoid detection, which can be costly.\n",
    "    - IPs can also quickly become blocked, rendering scraping scripts useless.\n",
    "3. Data Accuracy and Consistency Issues:\n",
    "    - Websites frequently update their layouts, URLs, or data structures.\n",
    "    - These changes require scrapers to be reconfigured frequently, increasing maintenance time ancost.\n",
    "    - Extracted data may contain inconsistencies, missing values, or irrelevant information that requires significant preprocessing before it becomes usable.\n",
    "    - Cleaning and standardizing such data can be time-intensive.\n",
    "    - Might require constant scraping and data refresh cycles\n",
    "4. Incompatibility with Dynamic and JavaScript-Heavy Content:\n",
    "    - Many modern websites use JavaScript frameworks (like React or Angular) that load content dynamically\n",
    "    - Scraping such content requires additional tools like Selenium or Puppeteer, which increase\n",
    "    complexity.\n",
    "    - JavaScript-heavy pages can be slower to load and scrape, making data extraction more timeconsuming and resource-demanding.\n",
    "5. Environmental Impact:\n",
    "    - Large-scale scraping operations consume substantial computational resources, which contributes to energy usage and, indirectly, environmental impact.\n",
    "    - This inadvertently translates to carbon emissions, an increasingly important consideration for environmentally conscious organizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043456d0-5472-4057-8831-8298049a8ea4",
   "metadata": {},
   "source": [
    "### Alternatives to Web Scraping\n",
    "1. Public APIs:\n",
    "    - Many websites offer public APIs that allow developers to access structured data directly.\n",
    "    - APIs provide clean and organized data formats, eliminating the need for extensive parsing or cleaning.\n",
    "    - Using an official API helps avoid legal risks associated with web scraping.\n",
    "2. RSS Feeds:\n",
    "    - Really Simple Syndication feeds are a way to automatically receive updates from websites in a single feed.\n",
    "    - RSS feeds are updated frequently, making it easy to access new content automatically.\n",
    "    - Since RSS feeds are structured in XML, they’re easy to parse and don’t require complex scraping scripts.\n",
    "3. Public Datasets:\n",
    "    - Data portals provide clean, verified, and well-documented datasets, which are typically updated periodically.\n",
    "    - Most data portals offer free access, with datasets available in formats like CSV, JSON, or Excel.\n",
    "    - Using existing datasets reduces time spent on collection and cleaning.\n",
    "4. Manual Data Collection:\n",
    "    - No technical setup or coding is needed, making it accessible to anyone who can access the site.\n",
    "    - Can be efficient without the need for dedicated tools or servers.\n",
    "    - It often avoids triggering anti-scraping measures.\n",
    "5. Licensed Partnerships with Data Owners:\n",
    "    - Partnerships can unlock data that is not available publicly, providing a competitive edge.\n",
    "    - Data is usually provided in structured formats and with reliable update frequencies, making it easy to integrate.\n",
    "    - Since data is obtained through agreements, this avoids any compliance issues\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaaabc8-0568-480b-be85-a5053dfa1e7f",
   "metadata": {},
   "source": [
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd380d1-7477-4c4f-8300-dd73c9f6fab1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23303249-c64f-4a9c-8f89-7555e7e265bc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e62ca4e-1e2c-4711-93d8-d711872aa963",
   "metadata": {},
   "source": [
    "# Client-Server model/architecture\n",
    "- The Client-Server model is a fundamental design framework for networked applications.\n",
    "- It organizes interactions between two major entities: clients (requesters) and servers (providers of resources).\n",
    "- This architecture underpins most modern networks, including the internet, web applications, email systems, and various enterprise systems.\n",
    "#### **Client:**\n",
    "- Device or an application that initiates requests for services or resources.\n",
    "- Clients are typically end-user devices (e.g., smartphones, laptops) or software applications (e.g., browsers, email clients) that communicate over a network.\n",
    "#### **Server:**\n",
    "- A server is a dedicated system or application that listens for and fulfills requests from clients.\n",
    "- Servers provide resources, data, or services to clients, typically through a network connection.\n",
    "#### **Working:**\n",
    "- importance of server for websites/apps\n",
    "- client-server communication\n",
    "\n",
    "## HTTP Request and Response\n",
    "### **HTTP:**\n",
    "- The HTTP (Hypertext Transfer Protocol) is the foundation of data communication on the web.\n",
    "- It facilitates the exchange of information between clients and servers.\n",
    "- The HTTP request-response cycle is central to how web applications function.\n",
    "#### **HTTP Request:**\n",
    "- An HTTP request is a message sent by the client to the server to initiate an action or request a resource.\n",
    "- The request message consists of following components:\n",
    "    - Request Line: Method / Verb, Address (URI), Version\n",
    "    - Headers: used for conveying additional meta-data about the request\n",
    "    - Body: contains the major contents of the request message\n",
    "<img src=\"WebScraping_img/http_request.jpg\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "<img src=\"WebScraping_img/request.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "#### **HTTP Response:**\n",
    "- An HTTP response is the message sent by the server back to the client after processing the request.\n",
    "- The response message consists of following components:\n",
    "  - Response Line: Status Code, Version\n",
    "  - Headers: used for conveying additional meta-data about the response\n",
    "  - Body: contains the requested contents (usually JSON format)\n",
    "<img src=\"WebScraping_img/http_response.jpg\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "<img src=\"WebScraping_img/response.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    " ## HTTP Method\n",
    "- HTTP methods, also known as HTTP verbs, are a fundamental part of the HTTP protocol.\n",
    "- They define the action to be performed on a resource identified by a URI.\n",
    "- Each method has specific semantics and is used for different purposes in client-server communication.\n",
    "1. GET:\n",
    "    - Used to request data from a specified resource.\n",
    "    - It is the most commonly used HTTP method.\n",
    "    - **Application:** Retrieving web pages, images, or other resources from a server.\n",
    "<img src=\"WebScraping_img/get.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "2. POST:\n",
    "    - Used to submit data to be processed to a specified resource.\n",
    "    - This often results in the creation of a new resource.\n",
    "    - **Application:** Submitting forms or uploading files\n",
    "<img src=\"WebScraping_img/post.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "3. PUT:\n",
    "    - Used to update an existing resource or create a new resource if it does not exist.\n",
    "    - It sends data to the server to replace the current representation of the resource.\n",
    "    - **Application:** Updating user details or replacing an entire user.\n",
    "<img src=\"WebScraping_img/put.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "4. PATCH:\n",
    "    - Used to apply partial modifications to a resource.\n",
    "    - It sends a set of instructions to update the resource rather than replacing it entirely.\n",
    "    - **Application:** Updating specific fields of a user, like changing a user’s email without altering other attributes.\n",
    "<img src=\"WebScraping_img/patch.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "5. DELETE:\n",
    "    - Used to remove a specified resource from the server.\n",
    "    - **Application:** Deleting user accounts, posts, or other resources.\n",
    "<img src=\"WebScraping_img/delete.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "Summary\n",
    "<img src=\"WebScraping_img/3.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "## HTTP Status Codes\n",
    "- HTTP status codes are three-digit numbers sent by a server in response to a client's request made to the server.\n",
    "- These codes are crucial for understanding the results of HTTP requests.\n",
    "- They provide important feedback to clients about the success or failure of requests and help developers diagnose issues.\n",
    "- Properly using and interpreting HTTP status codes is essential for effective communication between clients and servers in the web ecosystem.\n",
    "1. 1xx - Informational:\n",
    "    - These codes indicate that the request has been received and the process is continuing.\n",
    "    - They are rarely used in web applications but are important for certain protocols.\n",
    "    - **Examples:** 100 Continue, 101 Switching Protocols\n",
    "2. 2xx - Success:\n",
    "    - These codes indicate that the client's request was successfully received, understood, and accepted.\n",
    "    - **Examples:** 200 OK, 201 Created, 202 Accepted\n",
    "3. 3xx - Redirection:\n",
    "    - These codes indicate that further action is needed to complete the request.\n",
    "    - This usually involves redirection to another URI.\n",
    "    - **Examples:** 300 Multiple Choices, 301 Moved Permanently, 302 Found\n",
    "4. 4xx - Client Error:\n",
    "    - These codes indicate that the client made an error, resulting in the request being unable to be fulfilled.\n",
    "    - **Examples:** 400 Bad Request, 401 Unauthorized, 404 Not Found\n",
    "6. 5xx - Server Error:\n",
    "    - These codes indicate that the server failed to fulfill a valid request due to an error on the server side.\n",
    "    - **Examples:** 502 Bad Gateway, 503 Service Unavailable\n",
    "<img src=\"WebScraping_img/1.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "## Web Technologies\n",
    "- Web technologies encompass a wide array of tools, languages, and protocols used to create, maintain, and manage websites and web applications.\n",
    "- HTML provides the foundational structure, CSS handles visual presentation, and JavaScript adds dynamic behavior.\n",
    "- Together, they enable developers to build rich, interactive user experiences on the web.\n",
    "- Understanding these technologies is crucial for web scraping, as it allows developers to extract data from web pages effectively, even when dealing with dynamic content.\n",
    "1. HTML:\n",
    "    - HTML is the standard markup language used for creating web pages.\n",
    "    - It provides the structure and layout of a web document by defining elements like headings, paragraphs, links, images, and other types of content using appropriate tags.\n",
    "    - Tags can also be nested to create more complex structures.\n",
    "2. CSS:\n",
    "    - CSS is a stylesheet language used to describe the presentation of a document written in HTML.\n",
    "    - It controls the layout, colors, fonts, and overall visual appearance of web pages.\n",
    "    - CSS uses selectors to target HTML elements and apply styles.\n",
    "3. JavaScript:\n",
    "    - JavaScript is a high-level, dynamic programming language that enables interactivity and functionality on web pages.\n",
    "    - It allows developers to create rich user experiences through client-side scripting.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f698ef8d-ce8f-4ba4-a019-454c89491d54",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc275916-4e85-45b3-97aa-e7a2978ea4ed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846e1968-a957-4a16-bb56-44c8e498f5aa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da564023-4fe1-43ca-9b27-955add5ed216",
   "metadata": {},
   "source": [
    "# Anaconda\n",
    "- Anaconda is a popular open-source distribution of Python (and R) mainly used for data science, machine learning, and scientific computing.\n",
    "- Anaconda includes Conda, a package, dependency, and environment manager, making it easy to install and manage libraries and environments.\n",
    "- It comes with over 1,500 pre-installed packages for data science, including popular libraries like NumPy, Pandas, Matplotlib, TensorFlow, and Scikit-Learn.\n",
    "- Anaconda provides easy access to Jupyter Notebook, a powerful tool for interactive coding, data visualization, and exploratory analysis.\n",
    "- Anaconda includes conda, which is its package and environment manager (similar to pip)\n",
    "### Common Anaconda Prompts\n",
    "- conda update conda: **updates conda to the latest version**\n",
    "- conda update -all: **updates all packages to the latest version**\n",
    "- conda env list: **lists all available environments**\n",
    "- conda create --name \\<env_name>: **create a new environment**\n",
    "- conda activate \\<env_name>: **activates an environment**\n",
    "- conda deactivate: **deactivates the current working environment**\n",
    "- conda list: **lists all the packages installed in the current environment**\n",
    "- conda env export --name \\<env_name> --file environment.yml: **export an environment to a .yml file**\n",
    "- conda env create --file environment.yml: **import the exported environment in another system**\n",
    "- conda remove --name \\<env_name> --all: **removes the provided environment**\n",
    "\n",
    "### Setup\n",
    "1. Create a project directory\n",
    "2. Install Anaconda\n",
    "    - <a href=\"https://www.anaconda.com/\" target=\"_blank\"><b>Visit Official Anaconda Website</b></a>\n",
    "3. Open Anaconda Prompt\n",
    "4. Create an Anaconda environment\n",
    "5. Activate the created environment\n",
    "6. Install necessary packages:\n",
    "    - **pandas**\n",
    "    - **numpy**\n",
    "    - **requests**\n",
    "    - **beautifulsoup4**\n",
    "    - **lxml:** recommended for parsing XML/HTML content\n",
    "    - **html5lib:** alternate parsers for Beautiful Soup\n",
    "    - **selenium**\n",
    "    - **python-chromedriver-binary:** Chrome driver for Selenium\n",
    "    - **python-geckodriver:** Firefox driver for Selenium\n",
    "    - **webdriver-manager:** manages and downloads web drivers automatically (recommended, for auto-updates)\n",
    "    - **jupyter:** installs the main components of the Jupyter ecosystem\n",
    "    - **ipykernel:** to create a jupyter kernel for an environment\n",
    "6. Create an appropriate Jupyter kernel:\n",
    "    - **python -m ipykernel install --user --name=\\<env_name> --display-name \"\\<Your Env Display Name>\"**\n",
    "7. Launch Jupyter and create new notebooks using the appropriate kernel\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceabd846-406e-4dfb-809c-44e78570ff69",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2970a3-4b62-448f-934f-a7b9345bd571",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116b310e-25bb-434e-a791-74c96ff6604e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a71552-5bad-4fe0-8240-a9f66ea96889",
   "metadata": {},
   "source": [
    "# Requests\n",
    "- The Requests module is a powerful and user-friendly HTTP library for Python, designed to make it easier to send HTTP requests.\n",
    "- It abstracts away the complexities of making requests and handling responses, allowing developers to focus on building applications.\n",
    "\n",
    "### Key Features:\n",
    "- **Simplicity:** Known for its clean and straightforward syntax, making it accessible for beginners and experienced developers alike.\n",
    "- **Flexibility:** Supports a wide range of HTTP methods and allows for customization, such as adding headers, queryparameters, and more.\n",
    "- **User-Friendly:** Compared to the built-in urllib library, Requests provides a more intuitive API which is particularlybeneficial for rapid development and prototyping.\n",
    "- **Automatic Content Decoding:** Requests automatically decodes the content based on the response headers, so you can work with the data directly without worrying about the encoding.\n",
    "- **Session Management:** You can persist certain parameters across multiple requests using session objects, which can be useful for maintaining state.\n",
    "- **Built-in Error Handling:** Requests come with built-in mechanisms to handle common HTTP errors and exceptions.\n",
    "- **Community & Support:** The Requests library has a large and active community. Can easily find tutorials, documentation, and support for any issues you encounter.\n",
    "\n",
    "### Applications of Requests:\n",
    "1. **Web Scraping:**\n",
    "    - Requests is often the first step in web scraping.\n",
    "    - It allows developers to send HTTP requests to retrieve the HTML content of web pages, which can then be parsed and analyzed using libraries like Beautiful Soup or lxml.\n",
    "2. **API Interaction:**\n",
    "    - Commonly used to interact with RESTful APIs.\n",
    "    - It can send various types of HTTP requests (GET, POST, PUT, DELETE) to perform operations like retrieving data, creating new records, or updating existing ones.\n",
    "3. **File Uploads:**\n",
    "    - Requests supports file uploads, which is essential for applications where users need to submit files to a server.\n",
    "4. **Session Management:**\n",
    "    - Requests provides the ability to maintain sessions across multiple requests, which is useful for applications that require authentication.\n",
    "    - A web application that requires users to log in can use a session to maintain the user's login state while making subsequent requests.\n",
    "5. **Data Submission:**\n",
    "    - Requests can be used to submit data to web servers, especially in web forms where users enter information.\n",
    "    - Example: An application could use Requests to send user feedback or comments to a server\n",
    "\n",
    "### GET requests\n",
    "#### Query Parameters:\n",
    "- They are parameters which are included in the URI as part of the request message.\n",
    "- Mainly used to filter the data received from the server.\n",
    "- In a URI, these are placed after the ? and separated by &\n",
    "- **Application:** Filter phones by a price range in an e-commerce website\n",
    "\n",
    "### POST requests\n",
    "- Since POST requests are used to send data to the server to create a resource, the Requests library provides the data parameter for this\n",
    "- We can also pass JSON data directly using the json parameter\n",
    "- These parameters accept dictionary-like objects as arguments\n",
    "\n",
    "### Headers\n",
    "- Headers enable communicate of additional information such as authorization credentials, content types, and user-agent data, etc. between the clients and servers.\n",
    "- Understanding and managing headers is essential for authenticating requests, specifying response formats, handling cookies, and personalizing user experiences.\n",
    "\n",
    "#### Common types of Headers:\n",
    "- **Authorization:** Used for passing authentication tokens.\n",
    "- **Content-Type:** Specifies the format of the request data, such as JSON or XML.\n",
    "- **User-Agent:** Provides information about the client making the request, such as the browser or app.\n",
    "- **Accept:** Informs the server about the types of content the client can process, like JSON or HTML.\n",
    "- **Custom Headers:** Headers that may be unique to an API or web application.\n",
    "\n",
    "#### Practical tips for using Headers:\n",
    "- Always check API documentation to see required headers for a particular request.\n",
    "- Use headers for efficient server interaction because servers expect headers for security, data formatting, and client identification.\n",
    "- Perform appropriate error handling because missing or incorrect headers can lead to HTTP errors like 401 Unauthorized or 415 Unsupported Media Type.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b157c7-0aa1-43ff-b5f0-478a53d10ec0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "291db94d-4152-41bf-9248-01ef8608fa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.32.3\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "import requests\n",
    "## Version\n",
    "print(requests.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d0772c-4a71-4560-ac76-4f4603289d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET\n",
    "# send a request to GitHub API and printing the response\n",
    "# print the status code and the contents\n",
    "# GitHub API: `https://api.github.com`\n",
    "uri = \"https://api.github.com\"\n",
    "response = requests.get(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8290bd3b-f104-4637-80d3-8193a0371ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c91836b-0f72-43e1-9e21-edfc2a2f2477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 200 is success\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccbead4f-3687-4994-891d-7a2fb6316a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"current_user_url\":\"https://api.github.com/user\",\"current_user_authorizations_html_url\":\"https://github.com/settings/connections/applications{/client_id}\",\"authorizations_url\":\"https://api.github.com/authorizations\",\"code_search_url\":\"https://api.github.com/search/code?q={query}{&page,per_page,sort,order}\",\"commit_search_url\":\"https://api.github.com/search/commits?q={query}{&page,per_page,sort,order}\",\"emails_url\":\"https://api.github.com/user/emails\",\"emojis_url\":\"https://api.github.com/emojis\",\"events_url\":\"https://api.github.com/events\",\"feeds_url\":\"https://api.github.com/feeds\",\"followers_url\":\"https://api.github.com/user/followers\",\"following_url\":\"https://api.github.com/user/following{/target}\",\"gists_url\":\"https://api.github.com/gists{/gist_id}\",\"hub_url\":\"https://api.github.com/hub\",\"issue_search_url\":\"https://api.github.com/search/issues?q={query}{&page,per_page,sort,order}\",\"issues_url\":\"https://api.github.com/issues\",\"keys_url\":\"https://api.github.com/user/keys\",\"label_searc'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing first 1000 letters of the response\n",
    "response.content[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "789d0d9f-31f7-4502-b1be-e1ddc79d9a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use query parameters in a GET message\n",
    "# search for all GitHub repositories that contain the word `requests` and the main language used is `python`\n",
    "# GitHub repository search API: `https://api.github.com/search/repositories`\n",
    "uri = \"https://api.github.com/search/repositories\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ced36ab1-476c-455a-9a33-716e6172d2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"q\": \"requests+language:python\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b719cbb-4f56-48a2-a55e-419e0e4424e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(uri, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68599b99-12bc-458c-86ab-add688e697fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaf7d1a5-99d3-4d8b-93f8-39d8927116b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"total_count\":246,\"incomplete_results\":false,\"items\":[{\"id\":33210074,\"node_id\":\"MDEwOlJlcG9zaXRvcnkzMzIxMDA3NA==\",\"name\":\"secrules-language-evaluation\",\"full_name\":\"SpiderLabs/secrules-language-evaluation\",\"private\":false,\"owner\":{\"login\":\"SpiderLabs\",\"id\":508521,\"node_id\":\"MDEyOk9yZ2FuaXphdGlvbjUwODUyMQ==\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/508521?v=4\",\"gravatar_id\":\"\",\"url\":\"https://api.github.com/users/SpiderLabs\",\"html_url\":\"https://github.com/SpiderLabs\",\"followers_url\":\"https://api.github.com/users/SpiderLabs/followers\",\"following_url\":\"https://api.github.com/users/SpiderLabs/following{/other_user}\",\"gists_url\":\"https://api.github.com/users/SpiderLabs/gists{/gist_id}\",\"starred_url\":\"https://api.github.com/users/SpiderLabs/starred{/owner}{/repo}\",\"subscriptions_url\":\"https://api.github.com/users/SpiderLabs/subscriptions\",\"organizations_url\":\"https://api.github.com/users/SpiderLabs/orgs\",\"repos_url\":\"https://api.github.com/users/SpiderLabs/repos\",\"events_url\":\"https://api.github.com/users/SpiderLabs/events{/privacy}\",\"received_events_url\":\"https://api.github.com/users/SpiderLabs/received_events\",\"type\":\"Organization\",\"user_view_type\":\"public\",\"site_admin\":false},\"html_url\":\"https://github.com/SpiderLabs/secrules-language-evaluation\",\"description\":\"Set of Python scripts to perform SecRules language evaluation on a given http request.\",\"fork\":false,\"url\":\"https://api.github.com/repos/SpiderLabs/secrules-language-evaluation\",\"forks_url\":\"https://api.github.com/repos/SpiderLabs/secrules-language-evaluation/forks\",\"keys_url\":\"https://api.github.com/repos/SpiderLabs/secrules-language-evaluation/keys{/key_id}\",\"collaborators_url\":\"https://api.github.com/repos/SpiderLabs/secrules-language-evaluation/collaborators{/collaborator}\",\"teams_url\":\"https://api.github.com/repos/SpiderLabs/secrules-language-evaluation/teams\",\"hooks_url\":\"https://api.github.com/repos/SpiderLabs/secrules-language-evaluation/hooks\",\"issue_events_url\":\"https://api.github.com/repo'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e82d8a2e-c3be-4472-a12b-cdeb40fb855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## POST\n",
    "def get_data_format(rcvd_response):\n",
    "\tjson_response = rcvd_response.json()\n",
    "\tdata_format = json_response['headers']['Content-Type'].split(\"/\")[-1]\n",
    "\tprint(f\"Response data format: {data_format}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb59a127-3ab1-449d-afd2-1f9ab5b1d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send some data to a test server\n",
    "# server address: `https://httpbin.org/post`\n",
    "# https://httpbin.org/ is website which give us to illustrate these method on their website\n",
    "uri = \"https://httpbin.org/post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46db1e3f-55fa-4420-8fc6-8012ae7b6dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data we are sending\n",
    "data = {\n",
    "\t\"username\": \"bruce\",\n",
    "\t\"password\": \"bruce123\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e4cff87-10ad-40bf-aaab-8e7e217912b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(uri, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a0c5820-ce7d-42a4-a481-b6a50ddb64c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d81d6c3-8039-4f63-83b0-a98c45ca7cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'args': {},\n",
       " 'data': '',\n",
       " 'files': {},\n",
       " 'form': {'password': 'bruce123', 'username': 'bruce'},\n",
       " 'headers': {'Accept': '*/*',\n",
       "  'Accept-Encoding': 'gzip, deflate, zstd',\n",
       "  'Content-Length': '32',\n",
       "  'Content-Type': 'application/x-www-form-urlencoded',\n",
       "  'Host': 'httpbin.org',\n",
       "  'User-Agent': 'python-requests/2.32.3',\n",
       "  'X-Amzn-Trace-Id': 'Root=1-67d006a0-7d3f9a995c90278f31b3b6d2'},\n",
       " 'json': None,\n",
       " 'origin': '152.58.92.51',\n",
       " 'url': 'https://httpbin.org/post'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by default the data we sent is sent under form\n",
    "# server is telling it has received the information in the form of x-www-form-urlencoded\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9ece883-c3ca-4d11-8b2d-d5d1342733d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x-www-form-urlencoded'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()['headers']['Content-Type'].split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fe60e3e-77e8-40bc-a5fa-68959fc87188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response data format: x-www-form-urlencoded\n"
     ]
    }
   ],
   "source": [
    "get_data_format(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6675b0e1-27fe-4d2c-b151-4c0c4576f847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://httpbin.org/post'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# APIs usually expect data in JSON format\n",
    "# send data in JSON format using POST message\n",
    "uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81cd9e85-5c78-4d01-bed4-3fc062c57e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'username': 'bruce', 'password': 'bruce123'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb491008-693e-4cfc-8b9d-bfd61824c3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the data to json\n",
    "response2 = requests.post(uri, json=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fb2070c-1083-43e8-b208-970470506d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7104201d-15f9-4216-bf03-2e5cec5946a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'args': {},\n",
       " 'data': '{\"username\": \"bruce\", \"password\": \"bruce123\"}',\n",
       " 'files': {},\n",
       " 'form': {},\n",
       " 'headers': {'Accept': '*/*',\n",
       "  'Accept-Encoding': 'gzip, deflate, zstd',\n",
       "  'Content-Length': '45',\n",
       "  'Content-Type': 'application/json',\n",
       "  'Host': 'httpbin.org',\n",
       "  'User-Agent': 'python-requests/2.32.3',\n",
       "  'X-Amzn-Trace-Id': 'Root=1-67d006a1-2e2700e12410b46f0151d04f'},\n",
       " 'json': {'password': 'bruce123', 'username': 'bruce'},\n",
       " 'origin': '152.58.92.51',\n",
       " 'url': 'https://httpbin.org/post'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by default the data we sent is sent under data and json\n",
    "# server is telling it has received the information in the form of json\n",
    "response2.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33ab68bb-c811-4a20-8dfd-5450b01e1364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response data format: json\n"
     ]
    }
   ],
   "source": [
    "get_data_format(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f20ae39-3d27-4a06-b590-e9b046ce96a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUT\n",
    "# use PUT method to update data\n",
    "# address: `https://httpbin.org/put`\n",
    "uri = \"https://httpbin.org/put\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd89da44-d21e-4656-9b16-2c78583130a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "\t\"param1\": \"value1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "197f80df-5613-4825-814b-d7622a8a65b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.put(uri, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71f500f8-7854-46a8-82ca-47170dff9481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb9964d0-2226-4a14-8603-4a212bd75fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'args': {},\n",
       " 'data': '',\n",
       " 'files': {},\n",
       " 'form': {'param1': 'value1'},\n",
       " 'headers': {'Accept': '*/*',\n",
       "  'Accept-Encoding': 'gzip, deflate, zstd',\n",
       "  'Content-Length': '13',\n",
       "  'Content-Type': 'application/x-www-form-urlencoded',\n",
       "  'Host': 'httpbin.org',\n",
       "  'User-Agent': 'python-requests/2.32.3',\n",
       "  'X-Amzn-Trace-Id': 'Root=1-67d006a2-152bf6d2128277be146b4b17'},\n",
       " 'json': None,\n",
       " 'origin': '152.58.92.51',\n",
       " 'url': 'https://httpbin.org/put'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by default the data we sent is sent under form\n",
    "# server is telling it has received the information in the form of x-www-form-urlencoded\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32687b9c-a33d-4fea-87ef-2f57693cdb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE\n",
    "# use DELETE method to delete a resource\n",
    "# address: `https://httpbin.org/delete`\n",
    "uri = \"https://httpbin.org/delete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e074ef5-ac38-4463-a1ea-348111769883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No parameters will be passed\n",
    "response = requests.delete(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96601bc6-1a46-4285-a0a9-80834b9796b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e1b209d-59a4-48a0-86b7-ca224bfa1630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'args': {},\n",
       " 'data': '',\n",
       " 'files': {},\n",
       " 'form': {},\n",
       " 'headers': {'Accept': '*/*',\n",
       "  'Accept-Encoding': 'gzip, deflate, zstd',\n",
       "  'Content-Length': '0',\n",
       "  'Host': 'httpbin.org',\n",
       "  'User-Agent': 'python-requests/2.32.3',\n",
       "  'X-Amzn-Trace-Id': 'Root=1-67d006a3-697c3cab5c8e70a22b3f5b96'},\n",
       " 'json': None,\n",
       " 'origin': '152.58.92.51',\n",
       " 'url': 'https://httpbin.org/delete'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the form, data, json etc. Everything is empty as we have not passed any data.\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf11ae24-3c72-4611-8451-7eede39bc7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers\n",
    "# use a GET request\n",
    "# url: `https://httpbin.org/headers`\n",
    "headers = {\n",
    "\t'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',\n",
    "    'Accept': 'application/json',\n",
    "\t'Authorization': 'Bearer YOUR_ACCESS_TOKEN',\n",
    "\t'Content-Type': 'application/json',\n",
    "\t'X-Custom-Header': 'CustomValue',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e294084-da3b-460d-9382-d6bfc582fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"https://httpbin.org/headers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a3c3f90-87f6-41d7-ba9e-7a74c5af5ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(uri, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e8856a4-b2ba-4ef6-a94b-6e63bb81fe21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8265d8fe-1b4d-4ac3-8ae1-b11741ca12e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': 'Tue, 11 Mar 2025 09:47:17 GMT', 'Content-Type': 'application/json', 'Content-Length': '393', 'Connection': 'keep-alive', 'Server': 'gunicorn/19.9.0', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Credentials': 'true'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows the headers sent by the server to us\n",
    "response.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa08c34c-ab07-4197-8f55-8536e48ae9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.structures.CaseInsensitiveDict"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5c23612-4be2-4440-8dfb-db635cda21c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': 'Tue, 11 Mar 2025 09:47:17 GMT',\n",
       " 'Content-Type': 'application/json',\n",
       " 'Content-Length': '393',\n",
       " 'Connection': 'keep-alive',\n",
       " 'Server': 'gunicorn/19.9.0',\n",
       " 'Access-Control-Allow-Origin': '*',\n",
       " 'Access-Control-Allow-Credentials': 'true'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(response.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bdcb25e7-7b9f-47a2-9d94-0fa102a1f0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'headers': {'Accept': 'application/json',\n",
       "  'Accept-Encoding': 'gzip, deflate, zstd',\n",
       "  'Authorization': 'Bearer YOUR_ACCESS_TOKEN',\n",
       "  'Content-Type': 'application/json',\n",
       "  'Host': 'httpbin.org',\n",
       "  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',\n",
       "  'X-Amzn-Trace-Id': 'Root=1-67d006a5-108578ff4e271a884dc8cbf2',\n",
       "  'X-Custom-Header': 'CustomValue'}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It will show the headers we had sent to the server\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af7825ca-837c-48eb-8000-d3e9caf86681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accept          : application/json\n",
      "Accept-Encoding : gzip, deflate, zstd\n",
      "Authorization   : Bearer YOUR_ACCESS_TOKEN\n",
      "Content-Type    : application/json\n",
      "Host            : httpbin.org\n",
      "User-Agent      : Mozilla/5.0 (Windows NT 10.0; Win64; x64)\n",
      "X-Amzn-Trace-Id : Root=1-67d006a5-108578ff4e271a884dc8cbf2\n",
      "X-Custom-Header : CustomValue\n"
     ]
    }
   ],
   "source": [
    "# showing the our headers sent to the sever in formatted way\n",
    "for header, value in response.json()['headers'].items():\n",
    "\tprint(f\"{header:<16}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1aed6b0f-0194-4af8-82f7-d42d815215a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response Object\n",
    "# address: `https://api.github.com`\n",
    "# The most common attributes of the response object are:\n",
    "    # `status_code`: shows the HTTP status code of the request\n",
    "    # `text`: shows the content of the response as a string\n",
    "    # `content`: shows the content of the response as binary data\n",
    "    # `headers`: shows all the response headers\n",
    "    # `json()`: parse the server's response as JSON\n",
    "uri = \"https://api.github.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0585d0f-ac05-441e-86e4-5c139e21052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a686758-50dc-43a5-8662-ed3e6b39a92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "05489fbc-2cdd-437b-ac7f-4fa4ffe48f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5bcee850-0576-4495-a0b2-e3057a287400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4752538e-49ef-4eb4-9bae-1c1d7069a5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful Request!\n"
     ]
    }
   ],
   "source": [
    "if response.status_code == 200:\n",
    "\tprint(\"Successful Request!\")\n",
    "else:\n",
    "\tprint(\"Unsuccessful Request!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c903c160-b85c-43c1-9bef-2b4031b70fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"current_user_url\":\"https://api.github.com/user\",\"current_user_authorizations_html_url\":\"https://github.com/settings/connections/applications{/client_id}\",\"authorizations_url\":\"https://api.github.com/authorizations\",\"code_search_url\":\"https://api.github.com/search/code?q={query}{&page,per_page,sort,order}\",\"commit_search_url\":\"https://api.github.com/search/commits?q={query}{&page,per_page,sort,order}\",\"emails_url\":\"https://api.github.com/user/emails\",\"emojis_url\":\"https://api.github.com/emojis\",\"events_url\":\"https://api.github.com/events\",\"feeds_url\":\"https://api.github.com/feeds\",\"followers_url\":\"https://api.github.com/user/followers\",\"following_url\":\"https://api.github.com/user/following{/target}\",\"gists_url\":\"https://api.github.com/gists{/gist_id}\",\"hub_url\":\"https://api.github.com/hub\",\"issue_search_url\":\"https://api.github.com/search/issues?q={query}{&page,per_page,sort,order}\",\"issues_url\":\"https://api.github.com/issues\",\"keys_url\":\"https://api.github.com/user/keys\",\"label_search_url\":\"https://api.github.com/search/labels?q={query}&repository_id={repository_id}{&page,per_page}\",\"notifications_url\":\"https://api.github.com/notifications\",\"organization_url\":\"https://api.github.com/orgs/{org}\",\"organization_repositories_url\":\"https://api.github.com/orgs/{org}/repos{?type,page,per_page,sort}\",\"organization_teams_url\":\"https://api.github.com/orgs/{org}/teams\",\"public_gists_url\":\"https://api.github.com/gists/public\",\"rate_limit_url\":\"https://api.github.com/rate_limit\",\"repository_url\":\"https://api.github.com/repos/{owner}/{repo}\",\"repository_search_url\":\"https://api.github.com/search/repositories?q={query}{&page,per_page,sort,order}\",\"current_user_repositories_url\":\"https://api.github.com/user/repos{?type,page,per_page,sort}\",\"starred_url\":\"https://api.github.com/user/starred{/owner}{/repo}\",\"starred_gists_url\":\"https://api.github.com/gists/starred\",\"topic_search_url\":\"https://api.github.com/search/topics?q={query}{&page,per_page}\",\"user_url\":\"https://api.github.com/users/{user}\",\"user_organizations_url\":\"https://api.github.com/user/orgs\",\"user_repositories_url\":\"https://api.github.com/users/{user}/repos{?type,page,per_page,sort}\",\"user_search_url\":\"https://api.github.com/search/users?q={query}{&page,per_page,sort,order}\"}'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return string form\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc4faf62-75ad-4dd9-a165-e98a330ac0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is a string not dictionary\n",
    "type(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4f1a6d9-c95f-4da3-8278-095f7471aa1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"current_user_url\":\"https://api.github.com/user\",\"current_user_authorizations_html_url\":\"https://github.com/settings/connections/applications{/client_id}\",\"authorizations_url\":\"https://api.github.com/authorizations\",\"code_search_url\":\"https://api.github.com/search/code?q={query}{&page,per_page,sort,order}\",\"commit_search_url\":\"https://api.github.com/search/commits?q={query}{&page,per_page,sort,order}\",\"emails_url\":\"https://api.github.com/user/emails\",\"emojis_url\":\"https://api.github.com/emojis\",\"events_url\":\"https://api.github.com/events\",\"feeds_url\":\"https://api.github.com/feeds\",\"followers_url\":\"https://api.github.com/user/followers\",\"following_url\":\"https://api.github.com/user/following{/target}\",\"gists_url\":\"https://api.github.com/gists{/gist_id}\",\"hub_url\":\"https://api.github.com/hub\",\"issue_search_url\":\"https://api.github.com/search/issues?q={query}{&page,per_page,sort,order}\",\"issues_url\":\"https://api.github.com/issues\",\"keys_url\":\"https://api.github.com/user/keys\",\"label_search_url\":\"https://api.github.com/search/labels?q={query}&repository_id={repository_id}{&page,per_page}\",\"notifications_url\":\"https://api.github.com/notifications\",\"organization_url\":\"https://api.github.com/orgs/{org}\",\"organization_repositories_url\":\"https://api.github.com/orgs/{org}/repos{?type,page,per_page,sort}\",\"organization_teams_url\":\"https://api.github.com/orgs/{org}/teams\",\"public_gists_url\":\"https://api.github.com/gists/public\",\"rate_limit_url\":\"https://api.github.com/rate_limit\",\"repository_url\":\"https://api.github.com/repos/{owner}/{repo}\",\"repository_search_url\":\"https://api.github.com/search/repositories?q={query}{&page,per_page,sort,order}\",\"current_user_repositories_url\":\"https://api.github.com/user/repos{?type,page,per_page,sort}\",\"starred_url\":\"https://api.github.com/user/starred{/owner}{/repo}\",\"starred_gists_url\":\"https://api.github.com/gists/starred\",\"topic_search_url\":\"https://api.github.com/search/topics?q={query}{&page,per_page}\",\"user_url\":\"https://api.github.com/users/{user}\",\"user_organizations_url\":\"https://api.github.com/user/orgs\",\"user_repositories_url\":\"https://api.github.com/users/{user}/repos{?type,page,per_page,sort}\",\"user_search_url\":\"https://api.github.com/search/users?q={query}{&page,per_page,sort,order}\"}'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return binary form\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4c4d0819-aaed-460a-bc01-576d6068cc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is a bytes format neither dictionary nor string\n",
    "type(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d4047c6f-0a0b-4cad-b100-91d6c8bdd28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': 'Tue, 11 Mar 2025 09:47:09 GMT', 'Cache-Control': 'public, max-age=60, s-maxage=60', 'Vary': 'Accept,Accept-Encoding, Accept, X-Requested-With', 'ETag': '\"4f825cc84e1c733059d46e76e6df9db557ae5254f9625dfe8e1b09499c449438\"', 'x-github-api-version-selected': '2022-11-28', 'Access-Control-Expose-Headers': 'ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset', 'Access-Control-Allow-Origin': '*', 'Strict-Transport-Security': 'max-age=31536000; includeSubdomains; preload', 'X-Frame-Options': 'deny', 'X-Content-Type-Options': 'nosniff', 'X-XSS-Protection': '0', 'Referrer-Policy': 'origin-when-cross-origin, strict-origin-when-cross-origin', 'Content-Security-Policy': \"default-src 'none'\", 'Server': 'github.com', 'Content-Type': 'application/json; charset=utf-8', 'X-GitHub-Media-Type': 'github.v3; format=json', 'Content-Encoding': 'gzip', 'Accept-Ranges': 'bytes', 'X-RateLimit-Limit': '60', 'X-RateLimit-Remaining': '58', 'X-RateLimit-Reset': '1741690029', 'X-RateLimit-Resource': 'core', 'X-RateLimit-Used': '2', 'Content-Length': '510', 'X-GitHub-Request-Id': 'F1C5:3B80CA:361D8F:44237E:67D006A5'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows all the response headers (including what we headers we have sent and the additional headers server has added.)\n",
    "response.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "99928b06-fd8f-4f49-9f4d-ce2e5a28272a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.structures.CaseInsensitiveDict"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CaseInsensitiveDict \n",
    "type(response.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dd97ffa1-006c-435a-8f66-0338b4fafc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': 'Tue, 11 Mar 2025 09:47:09 GMT',\n",
       " 'Cache-Control': 'public, max-age=60, s-maxage=60',\n",
       " 'Vary': 'Accept,Accept-Encoding, Accept, X-Requested-With',\n",
       " 'ETag': '\"4f825cc84e1c733059d46e76e6df9db557ae5254f9625dfe8e1b09499c449438\"',\n",
       " 'x-github-api-version-selected': '2022-11-28',\n",
       " 'Access-Control-Expose-Headers': 'ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset',\n",
       " 'Access-Control-Allow-Origin': '*',\n",
       " 'Strict-Transport-Security': 'max-age=31536000; includeSubdomains; preload',\n",
       " 'X-Frame-Options': 'deny',\n",
       " 'X-Content-Type-Options': 'nosniff',\n",
       " 'X-XSS-Protection': '0',\n",
       " 'Referrer-Policy': 'origin-when-cross-origin, strict-origin-when-cross-origin',\n",
       " 'Content-Security-Policy': \"default-src 'none'\",\n",
       " 'Server': 'github.com',\n",
       " 'Content-Type': 'application/json; charset=utf-8',\n",
       " 'X-GitHub-Media-Type': 'github.v3; format=json',\n",
       " 'Content-Encoding': 'gzip',\n",
       " 'Accept-Ranges': 'bytes',\n",
       " 'X-RateLimit-Limit': '60',\n",
       " 'X-RateLimit-Remaining': '58',\n",
       " 'X-RateLimit-Reset': '1741690029',\n",
       " 'X-RateLimit-Resource': 'core',\n",
       " 'X-RateLimit-Used': '2',\n",
       " 'Content-Length': '510',\n",
       " 'X-GitHub-Request-Id': 'F1C5:3B80CA:361D8F:44237E:67D006A5'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(response.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dfc97546-248c-4921-a96c-75d6eec4f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date                               : Tue, 11 Mar 2025 09:47:09 GMT\n",
      "Cache-Control                      : public, max-age=60, s-maxage=60\n",
      "Vary                               : Accept,Accept-Encoding, Accept, X-Requested-With\n",
      "ETag                               : \"4f825cc84e1c733059d46e76e6df9db557ae5254f9625dfe8e1b09499c449438\"\n",
      "x-github-api-version-selected      : 2022-11-28\n",
      "Access-Control-Expose-Headers      : ETag, Link, Location, Retry-After, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Used, X-RateLimit-Resource, X-RateLimit-Reset, X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval, X-GitHub-Media-Type, X-GitHub-SSO, X-GitHub-Request-Id, Deprecation, Sunset\n",
      "Access-Control-Allow-Origin        : *\n",
      "Strict-Transport-Security          : max-age=31536000; includeSubdomains; preload\n",
      "X-Frame-Options                    : deny\n",
      "X-Content-Type-Options             : nosniff\n",
      "X-XSS-Protection                   : 0\n",
      "Referrer-Policy                    : origin-when-cross-origin, strict-origin-when-cross-origin\n",
      "Content-Security-Policy            : default-src 'none'\n",
      "Server                             : github.com\n",
      "Content-Type                       : application/json; charset=utf-8\n",
      "X-GitHub-Media-Type                : github.v3; format=json\n",
      "Content-Encoding                   : gzip\n",
      "Accept-Ranges                      : bytes\n",
      "X-RateLimit-Limit                  : 60\n",
      "X-RateLimit-Remaining              : 58\n",
      "X-RateLimit-Reset                  : 1741690029\n",
      "X-RateLimit-Resource               : core\n",
      "X-RateLimit-Used                   : 2\n",
      "Content-Length                     : 510\n",
      "X-GitHub-Request-Id                : F1C5:3B80CA:361D8F:44237E:67D006A5\n"
     ]
    }
   ],
   "source": [
    "for header, value in dict(response.headers).items():\n",
    "\tprint(f\"{header:<35}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9e46de3e-172c-4ee0-8f6a-3748b6d2de84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'current_user_url': 'https://api.github.com/user',\n",
       " 'current_user_authorizations_html_url': 'https://github.com/settings/connections/applications{/client_id}',\n",
       " 'authorizations_url': 'https://api.github.com/authorizations',\n",
       " 'code_search_url': 'https://api.github.com/search/code?q={query}{&page,per_page,sort,order}',\n",
       " 'commit_search_url': 'https://api.github.com/search/commits?q={query}{&page,per_page,sort,order}',\n",
       " 'emails_url': 'https://api.github.com/user/emails',\n",
       " 'emojis_url': 'https://api.github.com/emojis',\n",
       " 'events_url': 'https://api.github.com/events',\n",
       " 'feeds_url': 'https://api.github.com/feeds',\n",
       " 'followers_url': 'https://api.github.com/user/followers',\n",
       " 'following_url': 'https://api.github.com/user/following{/target}',\n",
       " 'gists_url': 'https://api.github.com/gists{/gist_id}',\n",
       " 'hub_url': 'https://api.github.com/hub',\n",
       " 'issue_search_url': 'https://api.github.com/search/issues?q={query}{&page,per_page,sort,order}',\n",
       " 'issues_url': 'https://api.github.com/issues',\n",
       " 'keys_url': 'https://api.github.com/user/keys',\n",
       " 'label_search_url': 'https://api.github.com/search/labels?q={query}&repository_id={repository_id}{&page,per_page}',\n",
       " 'notifications_url': 'https://api.github.com/notifications',\n",
       " 'organization_url': 'https://api.github.com/orgs/{org}',\n",
       " 'organization_repositories_url': 'https://api.github.com/orgs/{org}/repos{?type,page,per_page,sort}',\n",
       " 'organization_teams_url': 'https://api.github.com/orgs/{org}/teams',\n",
       " 'public_gists_url': 'https://api.github.com/gists/public',\n",
       " 'rate_limit_url': 'https://api.github.com/rate_limit',\n",
       " 'repository_url': 'https://api.github.com/repos/{owner}/{repo}',\n",
       " 'repository_search_url': 'https://api.github.com/search/repositories?q={query}{&page,per_page,sort,order}',\n",
       " 'current_user_repositories_url': 'https://api.github.com/user/repos{?type,page,per_page,sort}',\n",
       " 'starred_url': 'https://api.github.com/user/starred{/owner}{/repo}',\n",
       " 'starred_gists_url': 'https://api.github.com/gists/starred',\n",
       " 'topic_search_url': 'https://api.github.com/search/topics?q={query}{&page,per_page}',\n",
       " 'user_url': 'https://api.github.com/users/{user}',\n",
       " 'user_organizations_url': 'https://api.github.com/user/orgs',\n",
       " 'user_repositories_url': 'https://api.github.com/users/{user}/repos{?type,page,per_page,sort}',\n",
       " 'user_search_url': 'https://api.github.com/search/users?q={query}{&page,per_page,sort,order}'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows all the request headers in json\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ec97565c-ec15-4ae7-b253-e2550093b305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'current_user_url': 'https://api.github.com/user',\n",
       " 'current_user_authorizations_html_url': 'https://github.com/settings/connections/applications{/client_id}',\n",
       " 'authorizations_url': 'https://api.github.com/authorizations',\n",
       " 'code_search_url': 'https://api.github.com/search/code?q={query}{&page,per_page,sort,order}',\n",
       " 'commit_search_url': 'https://api.github.com/search/commits?q={query}{&page,per_page,sort,order}',\n",
       " 'emails_url': 'https://api.github.com/user/emails',\n",
       " 'emojis_url': 'https://api.github.com/emojis',\n",
       " 'events_url': 'https://api.github.com/events',\n",
       " 'feeds_url': 'https://api.github.com/feeds',\n",
       " 'followers_url': 'https://api.github.com/user/followers',\n",
       " 'following_url': 'https://api.github.com/user/following{/target}',\n",
       " 'gists_url': 'https://api.github.com/gists{/gist_id}',\n",
       " 'hub_url': 'https://api.github.com/hub',\n",
       " 'issue_search_url': 'https://api.github.com/search/issues?q={query}{&page,per_page,sort,order}',\n",
       " 'issues_url': 'https://api.github.com/issues',\n",
       " 'keys_url': 'https://api.github.com/user/keys',\n",
       " 'label_search_url': 'https://api.github.com/search/labels?q={query}&repository_id={repository_id}{&page,per_page}',\n",
       " 'notifications_url': 'https://api.github.com/notifications',\n",
       " 'organization_url': 'https://api.github.com/orgs/{org}',\n",
       " 'organization_repositories_url': 'https://api.github.com/orgs/{org}/repos{?type,page,per_page,sort}',\n",
       " 'organization_teams_url': 'https://api.github.com/orgs/{org}/teams',\n",
       " 'public_gists_url': 'https://api.github.com/gists/public',\n",
       " 'rate_limit_url': 'https://api.github.com/rate_limit',\n",
       " 'repository_url': 'https://api.github.com/repos/{owner}/{repo}',\n",
       " 'repository_search_url': 'https://api.github.com/search/repositories?q={query}{&page,per_page,sort,order}',\n",
       " 'current_user_repositories_url': 'https://api.github.com/user/repos{?type,page,per_page,sort}',\n",
       " 'starred_url': 'https://api.github.com/user/starred{/owner}{/repo}',\n",
       " 'starred_gists_url': 'https://api.github.com/gists/starred',\n",
       " 'topic_search_url': 'https://api.github.com/search/topics?q={query}{&page,per_page}',\n",
       " 'user_url': 'https://api.github.com/users/{user}',\n",
       " 'user_organizations_url': 'https://api.github.com/user/orgs',\n",
       " 'user_repositories_url': 'https://api.github.com/users/{user}/repos{?type,page,per_page,sort}',\n",
       " 'user_search_url': 'https://api.github.com/search/users?q={query}{&page,per_page,sort,order}'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7f66658f-57a4-4f7e-9097-e6513761bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with a public API\n",
    "# API: `https://jsonplaceholder.typicode.com/`\n",
    "# endpoint: `posts`\n",
    "# perform error handling: `raise_for_status()`\n",
    "# dummy post url\n",
    "url = \"https://jsonplaceholder.typicode.com/posts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c6287d9d-1a56-4252-88ea-5d0d01095bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "\n",
      "Successful GET request!\n",
      "\n",
      "Post 1:\n",
      "{'userId': 1, 'id': 1, 'title': 'sunt aut facere repellat provident occaecati excepturi optio reprehenderit', 'body': 'quia et suscipit\\nsuscipit recusandae consequuntur expedita et cum\\nreprehenderit molestiae ut ut quas totam\\nnostrum rerum est autem sunt rem eveniet architecto'}\n",
      "\n",
      "Post 2:\n",
      "{'userId': 1, 'id': 2, 'title': 'qui est esse', 'body': 'est rerum tempore vitae\\nsequi sint nihil reprehenderit dolor beatae ea dolores neque\\nfugiat blanditiis voluptate porro vel nihil molestiae ut reiciendis\\nqui aperiam non debitis possimus qui neque nisi nulla'}\n",
      "\n",
      "Post 3:\n",
      "{'userId': 1, 'id': 3, 'title': 'ea molestias quasi exercitationem repellat qui ipsa sit aut', 'body': 'et iusto sed quo iure\\nvoluptatem occaecati omnis eligendi aut ad\\nvoluptatem doloribus vel accusantium quis pariatur\\nmolestiae porro eius odio et labore et velit aut'}\n"
     ]
    }
   ],
   "source": [
    "# Make a GET request to retrieve a list of posts\n",
    "# error handling by using try and except.\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    # raise_for_status is the method to catch all the error that may occur while performing the conversation with the server and return none if no error\n",
    "    response.raise_for_status()\n",
    "# except will be executed only if there is error in the try block\n",
    "except Exception as e:\n",
    "    # Print the error if an exception occurs\n",
    "    print(e)\n",
    "else:\n",
    "    # Print the status code\n",
    "    status_code = response.status_code\n",
    "    print(f\"Status Code: {status_code}\")\n",
    "\n",
    "    # If the request was successful\n",
    "    if status_code == 200:\n",
    "        print(\"\\nSuccessful GET request!\")\n",
    "        # Parse response as JSON\n",
    "        posts = response.json()\n",
    "\n",
    "        # Print the top 3 posts\n",
    "        for i in range(3):\n",
    "            print(f\"\\nPost {i + 1}:\")\n",
    "            print(posts[i])\n",
    "    # run if the status code is other than 200\n",
    "    else:\n",
    "        print(\"Unsuccessful GET request!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "95d2200f-c8d9-4002-ab3b-ffc22f976cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a POST request to submit a post\n",
    "# include the following parameters:\n",
    "    # `title`\n",
    "    # `body`\n",
    "    # `userId` \n",
    "new_post = {\n",
    "\t\"title\": \"Sample Post\",\n",
    "\t\"body\": \"This is a sample post\",\n",
    "\t\"userId\": 101\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e2df9046-0087-4810-aad3-4b8783deed64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 201\n",
      "\n",
      "Successful POST request!\n",
      "\n",
      "Post:\n",
      "title          : Sample Post\n",
      "body           : This is a sample post\n",
      "userId         : 101\n",
      "id             : 101\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = requests.post(url, data=new_post)\n",
    "    response.raise_for_status()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "else:\n",
    "    status_code = response.status_code\n",
    "    # Run the code successfully for POST (201 Created)\n",
    "    if status_code == 201:\n",
    "        print(f\"Status Code: {status_code}\")\n",
    "        print(\"\\nSuccessful POST request!\")\n",
    "        post = response.json()\n",
    "        print(\"\\nPost:\")\n",
    "        for header, value in post.items():\n",
    "            print(f\"{header:<15}: {value}\")\n",
    "    else:\n",
    "        print(f\"Status Code: {status_code}\")\n",
    "        print(\"\\nUnsuccessful POST request!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4858e663-015e-4d39-86ce-b6542e9cf408",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c18758b-c892-49b8-8022-825f9a2a9c57",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc7d749-f224-458e-8add-3577caffae20",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf80ff7-f43f-4ae5-8eaa-a5c7fa170bcb",
   "metadata": {},
   "source": [
    "# Beautiful Soup\n",
    "- Beautiful Soup is a Python library used to parse HTML and XML documents.\n",
    "- It’s especially useful for web scraping because it helps navigate, search, and modify the HTML (or XML) content fetched from a webpage.\n",
    "- It transforms complex HTML documents into a tree structure, where each node corresponds to elements such as tags, text, attributes, etc.\n",
    "- This makes it easy to locate and extract specific information.\n",
    "\n",
    "### Advantages of Beautiful Soup:\n",
    "- **Easy to Learn and Use:** Has a user-friendly syntax that makes it easy for users to quickly locate and extract data from web pages.\n",
    "- **Flexible Parsing:** Works with different parsers, such as the built-in Python parser or lxml, offering flexibility in terms of speed and error handling.\n",
    "- **Handles Broken HTML:** Automatically fixes errors in the HTML structure, allowing users to scrape data from pages that other parsers might struggle with.\n",
    "- **Efficient Navigation and Search Functions:** Provides intuitive functions like find, find_all, and select to search and navigate through HTML tags and CSS selectors.\n",
    "- **Integration with Other Libraries:** Integrates smoothly with libraries like Requests, to retrieve web pages before parsing them. Also works well with - Pandas for data analysis or Selenium for JavaScriptheavy pages, making it a versatile choice for a complete web scraping workflow.\n",
    "- **Well-Documented and Active Community:** Has a comprehensive documentation and an active community that provides resources, tutorials, and troubleshooting support, making it accessible for new users.\n",
    "\n",
    "### Applications of Beautiful Soup:\n",
    "- **Price Comparison and Monitoring:** Widely used by e-commerce companies and consumers to scrape prices from various online stores.\n",
    "- **Job Listings Aggregation:** Commonly used to scrape job listings from platforms like LinkedIn, Indeed, or company career pages. This can help create job aggregators that compile positions from various sources.\n",
    "- **Market Research and Sentiment Analysis:** Companies often use web scraping to collect data from forums, blogs, and review sites to analyze customer sentiment about their products or their competitors.\n",
    "- **Real Estate Listings:** Useful for gathering real estate listings from sites like Zillow or Realtor.com. Data on prices, locations, features, and property availability can be scraped and analyzed to identify trends, track prices, and help potential buyers and real estate investors make informed decisions.\n",
    "- **Travel and Flight Price Tracking:** Used to monitor and compare prices across different airlines, hotels, and booking platforms. By gathering this data, users can develop apps to track flight and accommodation prices, helping travelers find the best deals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a57212-8abf-42d0-b734-6eaa8dadb52a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "54e42b3f-6e97-471f-868a-148eeeac8562",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d484c762-b58f-4912-8cbc-44267240911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a `soup` object\n",
    "with open(\"html-doc.html\") as file:\n",
    "    # here we are using html.parser but, we can other parsers also as per our convenience\n",
    "\tsoup = BeautifulSoup(file, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0916822b-e41d-4d62-822d-c543224f3e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is an object of BeautifulSoup\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a035ad22-8b20-45b7-ae5e-bbc2437965cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   The Dormouse's story\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p class=\"title\">\n",
      "   <b>\n",
      "    The Dormouse's story\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   Once upon a time there were three little sisters; and their names were\n",
      "   <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n",
      "    Elsie\n",
      "   </a>\n",
      "   ,\n",
      "   <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n",
      "    Lacie\n",
      "   </a>\n",
      "   and\n",
      "   <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">\n",
      "    Tillie\n",
      "   </a>\n",
      "   ;\n",
      "and they lived at the bottom of a well.\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   ...\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Basics of the `soup` object\n",
    "# `prettify()`\n",
    "# individual tags:\n",
    "    # `title`\n",
    "    # `a`\n",
    "    # `p`\n",
    "# `text`\n",
    "# `name`\n",
    "# `parent`\n",
    "# `children`\n",
    "# `descendants`\n",
    "# `get_text()`\n",
    "# `find()`\n",
    "# `find_all()`\n",
    "# `get()` / square bracket notation\n",
    "\n",
    "# prettifing the html code to look html code in simple and understanding view\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "192103ac-e923-4b0e-b5e3-82004724b480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>The Dormouse's story</title>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get a particular tag of the website\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "24b52027-5076-41b8-b29a-412950b27446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if there are multiple occurance of a tag then the first one will be returned\n",
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "15dbbafa-b3ae-421a-84a9-f72f8e692e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p class=\"title\"><b>The Dormouse's story</b></p>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "18808287-a801-4781-8165-ac766e17d4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<title>The Dormouse's story</title>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it is not a string so string operations will not work to get the content in the tag only\n",
    "print(type(soup.title))\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "29c35380-89c4-4e8f-85c5-0f5b850d6755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The Dormouse's story\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get the content of a tag and it is a string so we can apply all string operations on it\n",
    "print(type(soup.title.text))\n",
    "soup.title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6d022710-4481-4d42-8e57-7c51a63ddfa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE DORMOUSE'S STORY\n",
      "['The', \"Dormouse's\", 'story']\n",
      "the dormouse's story\n"
     ]
    }
   ],
   "source": [
    "print(soup.title.text.upper())\n",
    "print(soup.title.text.split())\n",
    "print(soup.title.text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1b7b3d2b-c561-4179-9fe3-97e3b36a6eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>The Dormouse's story</title>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "05a6463c-41e0-49ee-8f35-494a9dc813c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'title'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the name of the tag\n",
    "soup.title.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b5d52b37-b923-434d-84e5-7b9d8c11a7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<head><title>The Dormouse's story</title></head>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parent is used to get the parent tag of the current tag\n",
    "soup.title.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "138f7bb8-72ac-4f52-a148-80f3fa02135a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><head><title>The Dormouse's story</title></head>\n",
       "<body>\n",
       "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
       "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
       "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
       "and they lived at the bottom of a well.</p>\n",
       "<p class=\"story\">...</p></body></html>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parent of parent tag of current tag\n",
    "soup.title.parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "eeff707a-94e7-4cad-a985-965e006c9d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body>\n",
       "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
       "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
       "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
       "and they lived at the bottom of a well.</p>\n",
       "<p class=\"story\">...</p></body>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "30032b9c-9964-4b6c-be3c-11b5762e4438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<list_iterator at 0x24bab3202b0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# children of the current tag # It is returning list iterator instead of tag as there is no children of the tag but list_iterator \n",
    "soup.body.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "209085c7-b775-4183-9964-35a47343f09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
      "\n",
      "\n",
      "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
      "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
      "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
      "and they lived at the bottom of a well.</p>\n",
      "\n",
      "\n",
      "<p class=\"story\">...</p>\n"
     ]
    }
   ],
   "source": [
    "# iterating on the list_iterator \n",
    "for child in soup.body.children:\n",
    "\tprint(child) # there are three p tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "534c3985-49ad-43cb-b544-53f8ccc5e29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>The Dormouse's story</title>\n"
     ]
    }
   ],
   "source": [
    "# title is only children of the head tag\n",
    "for child in soup.head.children:\n",
    "\tprint(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9db33679-1c84-4dd7-b8f1-1f65f20b807c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Tag.descendants at 0x0000024BAAFB32A0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# descendant returns each and every tags and contents inside the tag recursivel. No matter how deeply the tags are\n",
    "soup.body.descendants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2c01b869-4f4b-4e8c-bd21-837b967c39f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
      "<b>The Dormouse's story</b>\n",
      "The Dormouse's story\n",
      "\n",
      "\n",
      "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
      "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
      "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
      "and they lived at the bottom of a well.</p>\n",
      "Once upon a time there were three little sisters; and their names were\n",
      "\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
      "Elsie\n",
      ",\n",
      "\n",
      "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>\n",
      "Lacie\n",
      " and\n",
      "\n",
      "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n",
      "Tillie\n",
      ";\n",
      "and they lived at the bottom of a well.\n",
      "\n",
      "\n",
      "<p class=\"story\">...</p>\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "for descendant in soup.body.descendants:\n",
    "\tprint(descendant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a34452fd-2c6f-44e8-9856-365b5236f58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<body>\n",
       "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
       "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
       "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
       "and they lived at the bottom of a well.</p>\n",
       "<p class=\"story\">...</p></body>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_text() is used to get only the plan text (in python string) while ignoring all tags\n",
    "soup.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6e5fa5ec-aad3-428b-92c1-bfedd3c10814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe Dormouse's story\\nOnce upon a time there were three little sisters; and their names were\\nElsie,\\nLacie and\\nTillie;\\nand they lived at the bottom of a well.\\n...\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.body.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b2eee6f5-40da-4fd6-9159-310e0e0bd0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dormouse's story\n",
      "Once upon a time there were three little sisters; and their names were\n",
      "Elsie,\n",
      "Lacie and\n",
      "Tillie;\n",
      "and they lived at the bottom of a well.\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# strip is used to remove the whitespaces if any\n",
    "print(soup.body.get_text().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5abca63c-ee3c-4779-9d81-d3693e456967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup.body.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "56d80d99-b447-4cdd-855f-5e1b5c81cb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Dormouse's story\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test() and get_text() will work Adjectively same if there is no tag in side the current tag\n",
    "soup.title.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dcb4cfda-7c31-4092-94a6-255fb94aaeb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fd76715f-d87f-46e2-ac14-5dafde86359e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find() is used to find the first occurance of any tag\n",
    "soup.find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a73cd0b3-a180-4a03-9f48-e557393035d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find_all() is used to find the all occurance of any ta\n",
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5244b35a-6448-41b1-a56e-e33cddadef32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is ResultSet, a list of anchor ⚓ tags\n",
    "type(soup.find_all('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4c4a3328-19ea-4033-aabb-730da3e54422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'link1'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get() to access value of any key\n",
    "# method 1\n",
    "soup.a['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fac7a5a6-4af8-454c-88e5-6c4b3b6f18f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sister']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get()\n",
    "# method 2 (advisable to use instead of method 1)\n",
    "soup.a.get('class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1dacaa0b-9470-4d08-a2c1-f52fa4a59555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://example.com/elsie'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.a.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "15aa84c9-0c49-45e1-aca9-1f100786247a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'link1'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.a.get('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8b1f73-d11e-45e2-af4b-42f3a1688ced",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3290919d-0323-4293-9394-0674114fe308",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e5971b-54a6-4d58-ae04-183aca40eeab",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a57e57-3aaa-4b4d-a78e-7d5359e4b782",
   "metadata": {},
   "source": [
    "# What is Selenium?\n",
    "- Selenium is a powerful, open-source tool used for automating web browsers.\n",
    "- It is often utilized for web scraping when interacting with dynamic websites that rely on JavaScript to load content, which static scraping libraries like Beautiful Soup or Requests cannot handle effectively.\n",
    "- When scraping websites using Python, Selenium acts as a web driver, automating browser actions to interact with web pages like a human user.\n",
    "- It can navigate to web pages, simulate user interactions (clicks, scrolls, form submissions), and extract data directly from rendered HTML.\n",
    "- Selenium was originally developed for testing web applications. Over time, it became a popular tool for web scraping due to its ability to handle dynamic, JavaScript-heavy websites.\n",
    "\n",
    "### Key Features:\n",
    "- Dynamic Content Handling:\n",
    "    - Can scrape data from JavaScript-heavy sites.\n",
    "    - Waits for elements to load before interacting or extracting data.\n",
    "- Interaction Simulation:\n",
    "    - Handles tasks such as clicking buttons, filling forms, selecting dropdowns, and scrolling pages.\n",
    "    - Useful for scraping data hidden behind user interactions.\n",
    "- Cross-Browser Support:\n",
    "    - Works with popular browsers like Chrome, Firefox, Edge, and Safari.\n",
    "- Customizable Waits:\n",
    "    - Implements explicit and implicit waits to ensure elements are fully loaded before actions are performed.\n",
    "\n",
    "###  Comparison\n",
    "<img src=\"WebScraping_img/5.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "\n",
    "### Advantages:\n",
    "- Handles JavaScript and AJAX (Asynchronous JavaScript and XML)\n",
    "- Simulates user behavior\n",
    "- Allows scraping data behind login screens or requiring user interaction\n",
    "\n",
    "### Disadvantages:\n",
    "- Slower than static scraping methods since it requires a full browser environment\n",
    "- Heavy CPU and memory usage\n",
    "- Websites may block or detect Selenium bots\n",
    "\n",
    "# Navigating Web Pages Steps\n",
    "1. Activate virtual environment\n",
    "2. Install selenium\n",
    "3. Download suitable web driver\n",
    "\n",
    "## 1. Locating Elements\n",
    "- Selenium provides the function **find_element** to find and locate elements of a webpage\n",
    "- There're several strategies to identify an element of a webpage:\n",
    "1. ID:\n",
    "<img src=\"WebScraping_img/6.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "2. Name:\n",
    "<img src=\"WebScraping_img/7.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "3. Class:\n",
    "<img src=\"WebScraping_img/8.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "4. Tag:\n",
    "<img src=\"WebScraping_img/9.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "5. XPath:\n",
    "<img src=\"WebScraping_img/10.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "6. Element Location Strategies:\n",
    "<img src=\"WebScraping_img/11.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "<img src=\"WebScraping_img/12.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "## 2. XPath\n",
    "- What is XPath?\n",
    "    - XPath (XML Path Language) is a query language used to navigate and locate elements within XML or HTML documents.\n",
    "    - Selenium uses XPath as one of its locator strategies to find elements on a webpage.\n",
    "    - XPath is a powerful tool for locating elements in Selenium, offering unmatched flexibility and precision.\n",
    "    - It's a go-to solution when working with complex web pages or when other locators are insufficient.\n",
    "### Advantages:\n",
    "- Locate Elements Anywhere:\n",
    "    - XPath can traverse the entire DOM, allowing you to locate elements in deep nested structures or without unique identifiers.\n",
    "    - Works for all elements, even those without **id**, **name**, or **class**.\n",
    "- Offers Rich Syntax: XPath supports a variety of conditions and operators, enabling users to\n",
    "    - Match elements by attributes\n",
    "    - Locate elements by position\n",
    "    - Use partial matches\n",
    "- Supports Text-Based Matching:\n",
    "    - Locate elements based on visible text\n",
    "- Supports Relative Paths: You can locate elements without specifying their full path in the DOM, making XPath expressions robust to changes\n",
    "    - Relative: **//div\\[@class='example']**\n",
    "    - Absolute: **/html/body/div\\[1]/div\\[2]**\n",
    "- Navigate the DOM Hierarchy\n",
    "<img src=\"WebScraping_img/13.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "- Independent of HTML Structure\n",
    "    - XPath can navigate through the DOM and locate elements that might not be directly visible or styled.\n",
    "    - Elements in the DOM can exist even if they are hidden from the user's view, such as elements with CSS properties like **display: none** or **visibility: hidden**\n",
    "\n",
    "### Disadvantages:\n",
    "- XPath is slower than CSS Selectors because of its ability to traverse the entire DOM.\n",
    "- XPath expressions can be harder to read and maintain, especially for deeply nested elements.\n",
    "- Some older browsers may have limited support for advanced XPath queries.\n",
    "\n",
    "## 3. Interacting with Elements\n",
    "1. Typing Input into Fields:\n",
    "    - This is achieved using the **send_keys** function\n",
    "    - It's used to simulate typing into an element, such as a text input field or a text area.\n",
    "    - It allows users to send keystrokes or input text programmatically as if a user were typing on a keyboard.\n",
    "    - Enables simulation of key presses like Enter, Tab, etc.\n",
    "        - This is achieved by using the class **Keys** from the module **selenium.webdriver.common.keys**\n",
    "2. Clearing Input Fields:\n",
    "    - This is achieved using the **clear** function\n",
    "    - It's used to clear the text content of a text input element on a web page\n",
    "    - It ensures the field is empty before entering new data (which can be done using the send_keys function)\n",
    "3. Clicking Buttons:\n",
    "    - This is achieved using the **click** function\n",
    "    - It's used to simulate a mouse click on a web element\n",
    "    - Allows users to interact with clickable elements on a web page, such as buttons, links, checkboxes, or radio buttons.\n",
    "4. Submitting Forms:\n",
    "    - This is achieved using the **submit** function\n",
    "    - Helps to automatically submit a form without explicitly clicking the \"Submit\" button\n",
    "    - Executes the action associated with the \\<form> tag, such as navigating to a new page or processing data.\n",
    "    - Direct use of submit is not common in modern web development:\n",
    "        - Most forms today rely on JavaScript events or custom logic\n",
    "        - For such forms, using the click function on a \"Submit\" button or JavaScript execution may be more reliable.\n",
    "\n",
    "## 4. Dropdown & Multiselect\n",
    "1. Dropdown:\n",
    "    - Need to identify the dropdown element as usual\n",
    "    - Wrap the identified element under the class **Select**, imported from the module **selenium.webdriver.support.select**\n",
    "    - There are 3 ways to select an option from the dropdown list:\n",
    "        - **select_by_index:** provide the index of the option\n",
    "        - **select_by_value:** provide the name attribute of the option\n",
    "        - **select_by_visible_text:** provide the actual text value of the option\n",
    "2. Multiselect:\n",
    "    - Similar to dropdown as discussed above\n",
    "    - There are multiple ways to select an option:\n",
    "        - **select_by_index:** provide the index of the option\n",
    "        - **select_by_value:** provide the name attribute of the option\n",
    "        - **select_by_visible_text:** provide the actual text value of the option\n",
    "    - Similarly, there are multiple ways to deselect an option:\n",
    "        - **deselect_by_index:** provide the index of the option\n",
    "        - **deselect_by_value:** provide the name attribute of the option\n",
    "        - **deselect_by_visible_text:** provide the actual text value of the option\n",
    "        - **deselect_all:** takes no arguments\n",
    "\n",
    "## 5. Scrolling\n",
    "- There're several ways of scrolling a webpage using Selenium:\n",
    "    - Scrolling to a specific element\n",
    "    - Scrolling vertically\n",
    "    - Scrolling horizontally\n",
    "    - Scrolling the page height\n",
    "    - Infinite scrolling\n",
    "- Scrolling actions are mainly achieved using the **execute_script** method\n",
    "    - This method is mainly used to execute JavaScript code within the context of the currently loaded webpage\n",
    "    - It allows users to directly interact with and manipulate the Document Object Model (DOM) of the page\n",
    "    - Helps interact with elements that might not be accessible using Selenium's standard methods\n",
    "    - To better handle dynamically loaded content on modern and JavaScript-heavy websites\n",
    "        - **script:** String containing JavaScript code\n",
    "        - **args:** Optional arguments to pass to the JavaScript code, usually Web Elements or other variables\n",
    "<img src=\"WebScraping_img/14.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "\n",
    "1. Scrolling to a specific element:\n",
    "    - Identify any element on the webpage\n",
    "    - Use the method **scrollIntoView**\n",
    "<img src=\"WebScraping_img/15.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "2. Scrolling Vertically:\n",
    "    - Use the method **scrollBy**\n",
    "    - Specify the number pixels to scroll by\n",
    "    - +ve value indicates scrolling down\n",
    "    - -ve value indicates scrolling up\n",
    "<img src=\"WebScraping_img/16.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "3. Scrolling Horizontally:\n",
    "    - Similar to scrolling vertically\n",
    "    - The no. of pixels are provided as the first argument\n",
    "    - +ve value indicates scrolling to the right\n",
    "<img src=\"WebScraping_img/17.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "4. Scrolling to Page Height:\n",
    "    - Use the **scrollTo** method\n",
    "    - Pass the value **document.body.scrollHeight** in place of pixels as usual\n",
    "    - It refers to total height of the content in a webpage\n",
    "<img src=\"WebScraping_img/18.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "5. Infinite Scrolling:\n",
    "    - Initially, the webpage loads a fixed amount of content.\n",
    "    - As the user scrolls close to the bottom of the page, a JavaScript function triggers a request to load more content dynamically\n",
    "    - The new content is added to the page, and the process repeats.\n",
    "###### Algorithm:\n",
    "- Get the height of the currently loaded page (h1)\n",
    "- Run an infinite loop\n",
    "- Scroll down the page to h1\n",
    "- Inside the loop, get the height of the page again (h2)\n",
    "- If h1 is same as h2, break out of the loop as no new content has been loaded\n",
    "- If h1 is not same as h2, update h1 as h2 and continue the loop\n",
    "<img src=\"WebScraping_img/19.png\" width=\"1000px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443c3182-5eae-4155-bd11-f204abe57a44",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c707a45c-4549-4d02-99cb-31e383536cc4",
   "metadata": {},
   "source": [
    "## 1. Advanced Web Interactions Intro\n",
    "- This section covers advanced web interactions that go beyond basic navigation and element manipulation.\n",
    "- By mastering these techniques, developers will be able to handle real-world web scraping and automation challenges, including interacting with dynamic content, handling alerts, and managing iframes.\n",
    "\n",
    "## 2. Explicit Waits\n",
    "#### What is Explicit Wait?\n",
    "- Type of wait that pauses the execution of the script until a specific condition is met or a specified maximum time is reached .\n",
    "- Useful when dealing with dynamic web elements that take time to appear or become interactable on the page.\n",
    "- Helps avoid exceptions such as **NoSuchElementException** or **ElementNotInteractableException**\n",
    "- Improves script reliability by waiting only as long as necessary\n",
    "- Optimizes test execution time compared to fixed delays (e.g., time.sleep())\n",
    "\n",
    "#### How to Implement?\n",
    "- Selenium provides the **WebDriverWait** class to implement explicit waits\n",
    "- It works with expected conditions defined in the **selenium.webdriver.support.expected_conditions module**\n",
    "- The script checks for the condition at regular intervals (default - 500ms) until it's met or the timeout occurs\n",
    "<img src=\"WebScraping_img/20.png\" width=\"1000px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "- Parameters:\n",
    "    - **driver:** The WebDriver instance controlling the browser\n",
    "    - **timeout:** Maximum time (in seconds) to wait for the condition to be met\n",
    "    - **poll_frequency:** How often (in seconds) the condition is checked (default: 0.5 seconds)\n",
    "    - **ignored_exceptions:** A tuple of exceptions to ignore while waiting (optional)\n",
    "- WebDriverWait often works in conjunction with **Expected Conditions** (EC) to define what to wait for:\n",
    "<img src=\"WebScraping_img/21.png\" width=\"800px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "\n",
    "## 3. Implicit Waits\n",
    "#### What is Implicit Wait?\n",
    "- Implicit Waits are a mechanism in Selenium to specify a default waiting time for the WebDriver when **searching for elements on a webpage**.\n",
    "- If an element is not immediately found, the WebDriver waits for the specified duration before throwing a **NoSuchElementException**.\n",
    "- This type of wait applies globally to **all element searches in the WebDriver instance**.\n",
    "#### How It Works:\n",
    "- We can set up the waiting duration using the **implicitly_wait** method of the webdriver instance.\n",
    "- Makes Selenium scripts resilient to minor delays in the loading of web elements caused by network speed, animations, or dynamic content.\n",
    "- Once set, it applies to all **find_element** and **find_elements** methods for the lifetime of the WebDriver instance.\n",
    "- If the element is found before the timeout period, the script proceeds immediately. Otherwise, it waits until the timeout is reached and raises an exception if the element is still not found.\n",
    "<img src=\"WebScraping_img/22.png\" width=\"800px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "#### Advantages:\n",
    "- **Simplicity:** Easy to implement and applies globally, avoiding repetitive waits for every element.\n",
    "- **Resilience:** Handles minor delays in loading dynamically generated elements, reducing script failures.\n",
    "- **Better Control:** Provides a default buffer for all element searches without the need for explicit handling.\n",
    "#### Disadvantages:\n",
    "- Since it applies globally, it may not suit situations where different elements require different wait times.\n",
    "- Mixing implicit waits with explicit waits can cause unpredictable behavior, as implicit waits can interfere with explicit wait polling mechanisms.\n",
    "- Implicit waits only handle element visibility or presence and cannot wait for specific conditions like page titles or JavaScript execution.\n",
    "#### Best Practices:\n",
    "- Use either implicit waits or explicit waits in your script, but not both, to avoid conflicts.\n",
    "- Set reasonable timeout durations and not very high implicit wait times (e.g., 60 seconds) as it can unnecessarily delay script execution.\n",
    "- Implicit waits are suitable for simple scripts without complex wait conditions.\n",
    "#### Implicit vs Explicit Wait:\n",
    "<img src=\"WebScraping_img/23.png\" width=\"1000px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "\n",
    "## 4. Frames & IFrames\n",
    "#### What is a Frame/IFrame?\n",
    "- In web development, **frames** and **iframes** are HTML elements that allow you to embed one HTML document inside another.\n",
    "#### Frame:\n",
    "- Part of the **\\<frame>** and **\\<frameset>** HTML tags, which were used in early web development to divide the browser window into multiple sections, each capable of loading a separate HTML document.\n",
    "- Now obsolete in HTML5, frames are rarely used. They were replaced by iframes and other modern layout techniques like CSS Grid and Flexbox.\n",
    "#### Iframe (Inline Frame):\n",
    "- An **\\<iframe>** is an HTML element that embeds another HTML document within the current page.\n",
    "- Changes in the parent page (like CSS or JavaScript) typically do not affect the iframe's content, and vice versa.\n",
    "- Each iframe has its own DOM (Document Object Model), CSS, and JavaScript scope.\n",
    "- Interaction between the parent and iframe is restricted if they originate from different domains for security reasons.\n",
    "#### Working with Selenium:\n",
    "- To interact with iframe content in Selenium, users must explicitly switch the context to the iframe using **driver.switch_to.frame()**\n",
    "- Frame content can be identified in many different ways:\n",
    "    - ID\n",
    "    - Name\n",
    "    - Index\n",
    "    - Xpath\n",
    "    - CSS Selector\n",
    "- After interacting with an iframe, switch back to the parent page using **driver.switch_to.default_content()**\n",
    "#### Best Practices:\n",
    "- Ensure you know which frame or iframe contains the desired elements by inspecting the page source.\n",
    "- Whenever possible, avoid switching by index to maintain flexibility if the page structure changes, Only use **XPATH**.\n",
    "- Always switch back to the main content after interacting with a frame.\n",
    "\n",
    "## 5. Handling Alerts\n",
    "#### What are Alerts:\n",
    "- Alerts in web refer to small, temporary messages or pop-ups that appear in a web browser to communicate information or request user actions.\n",
    "- They are typically generated by JavaScript or built into the HTML/CSS structure of a webpage.\n",
    "- Alerts are used for various purposes, including notifying users, obtaining confirmation, or prompting for input.\n",
    "#### Types of Alerts:\n",
    "1. JavaScript alerts:\n",
    "    - Created using JavaScript's **alert()** function.\n",
    "    - Displays a simple message to the user with a single \"OK\" button.\n",
    "    - Blocks user interaction with the page until dismissed.\n",
    "2. Confirmation Alerts:\n",
    "    - Created using JavaScript's **confirm()** function.\n",
    "    - Asks the user to confirm an action with \"OK\" and \"Cancel\" buttons.\n",
    "    - Returns true for \"OK\" and false for \"Cancel\"\n",
    "3. Prompt Alerts:\n",
    "    - Created using JavaScript's **prompt()** function.\n",
    "    - Requests user input and provides an input field along with \"OK\" and \"Cancel\" buttons.\n",
    "    - Returns the input value for \"OK\" or null for \"Cancel\".\n",
    "4. Browser-Based Alerts (Authentication Pop-Ups):\n",
    "    - Appear when a website requests HTTP basic authentication.\n",
    "    - Requires entering a username and password\n",
    "5. Custom HTML Alerts (Modal Dialogs):\n",
    "    - Designed using HTML, CSS, and JavaScript to create custom alert-style messages.\n",
    "    - Offers more flexibility in design and functionality (e.g., styled dialog boxes with multiple buttons or inputs).\n",
    "#### Handling Alerts with Selenium:\n",
    "- To interact with alerts in Selenium, users must explicitly switch the context to the alert box using **driver.switch_to.alert**\n",
    "    - Use the **accept()** method to click the \"OK\" button\n",
    "    - Use the **dismiss()** method to click the \"Cancel\" button on confirmation pop-ups\n",
    "    - Use the **text** attribute to retrieve the message displayed on the alert\n",
    "    - Use the **send_keys()** method to input text into a prompt pop-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257932c9-aa6e-4141-838a-bc5c76b9f6f1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a33a5ae-0ae6-4887-9aeb-a585cd60c956",
   "metadata": {},
   "source": [
    "## Best Practices and Optimization Intro\n",
    "- This section provides strategies for writing maintainable, efficient, and robust Selenium scripts.\n",
    "- Adhering to these best practices will improve the performance of test automation or web scraping projects and make the code easier to debug and maintain.\n",
    "#### 1. Write Maintainable Code\n",
    "1. Page Object Model (POM):\n",
    "    - The Page Object Model is a design pattern that separates the code used to locate and interact with web elements.\n",
    "    - This improves code readability and reusability\n",
    "    - Each webpage is represented by a class\n",
    "    - Elements are defined as variables, and interactions (methods) are encapsulated within the class\n",
    "2. Variable Names:\n",
    "    - Avoid overly generic names (ex: element1, button1)\n",
    "    - Use descriptive names to define web elements\n",
    "#### 2. Enhance Performance\n",
    "1. Using Appropriate Waits:\n",
    "    - Overuse of time.sleep() can slow down scripts unnecessarily\n",
    "    - Look to use Explicit Waits and Implicit Waits for better performance\n",
    "    - Use Explicit Waits for better control\n",
    "2. Optimize Locator Strategies:\n",
    "    - Use ID and NAME where possible as they are the fastest locators\n",
    "    - Avoid using XPath unless necessary, as it can be slower\n",
    "3. Reuse Browser Sessions:\n",
    "    - Instead of launching a new browser for each test, reuse the browser session if applicable\n",
    "    - For scraping, minimize browser interaction by using headless mode\n",
    "<img src=\"WebScraping_img/24.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "#### 2. Robustness and Error Handling\n",
    "1. Try-Except blocks:\n",
    "    - Wrap code for critical interactions within try-except blocks to manage unexpected failures\n",
    "    - Catch specific exceptions if possible\n",
    "<img src=\"WebScraping_img/25.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "2. Release Resources:\n",
    "    - Always close the browser session at the end of the script to free resources\n",
    "    - This can be achieved using driver.quit()\n",
    "#### 4. Logging and Debugging\n",
    "1. Logging:\n",
    "    - Avoid printing directly to the console to debug; use logs instead\n",
    "    - Use python’s logging module for better control\n",
    "<img src=\"WebScraping_img/26.png\" width=\"500px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "2. Capture Screenshots:\n",
    "    - Save a screenshot for debugging If code fails\n",
    "    - Use the save_screenshot method of the webdriver instance\n",
    "3. Developer Tools:\n",
    "    - Use the browser's Developer Tools to inspect element locators and understand dynamic content of the webpage\n",
    "    - Gives a better understanding of the website structure and its respective code\n",
    "#### 5. Security Considerations\n",
    "1. Secure Sensitive Data:\n",
    "    - Avoid exposing credentials in scripts\n",
    "    - Use encrypted files or environment variables for storage\n",
    "2. Respect Website Policies:\n",
    "    - Check the website’s robots.txt and Terms of Service before scraping\n",
    "    - Be ethical in your automation and scraping practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1edf4be-2aa8-48ed-b7b2-2fb8bc6ac321",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb73e2-5409-4fdb-ae49-6ed9a17137a2",
   "metadata": {},
   "source": [
    "## Action Chains:\n",
    "- Action Chains in Selenium is a feature that allows automating complex user interactions such as mouse and keyboard events.\n",
    "- It is part of the **ActionChains** class in Selenium, designed to handle actions like clicking, dragging, hovering, and sending keyboard input.\n",
    "#### Working of Action Chains:\n",
    "- An instance of ActionChains is created by passing the WebDriver instance.\n",
    "- Use various methods provided by the class to define a sequence of actions.\n",
    "- Call the **perform()** method to execute all actions in the defined sequence.\n",
    "#### Common Methods:\n",
    "- **click()**: Clicks on a specified web element.\n",
    "- **click_and_hold()**: Clicks without releasing the mouse button.\n",
    "- **double_click()**: Double-clicks on the element.\n",
    "- **context_click()**: Right-clicks on the element.\n",
    "- **move_to_element()**: Moves the mouse pointer to the element.\n",
    "#### Loading a Webpage:\n",
    "- We can use **return document.readyState** to retrieve the current loading state of the web page.\n",
    "- The values returned by this command can be:\n",
    "    - **loading**: The document is still loading.\n",
    "    - **interactive**: The document has been loaded but external resources like images and stylesheets might still be loading.\n",
    "    - **complete**: The entire document, including external resources, has been fully loaded.\n",
    "- This is commonly used in Selenium scripts to wait until the page is fully loaded before interacting with elements.\n",
    "#### until():\n",
    "- It has a parameter **method**.\n",
    "- This parameter must be a callable (function or lambda) which takes a **WebElement** as input and returns a boolean value.\n",
    "- If the callable doesn’t return a truthy value within the timeout period, a **TimeoutException** exception will be raised.\n",
    "\n",
    "## \"This site can't be reached\":\n",
    "<img src=\"WebScraping_img/site-cant-be-reached.png\" width=\"800px\" style=\"display: block; margin: auto;\">\n",
    "<br>\n",
    "\n",
    "#### Causes:\n",
    "- The ChromeDriver or WebDriver version might not match the installed browser version\n",
    "- A proxy or VPN might interfere with the request\n",
    "- Firewall or corporate network restrictions may block access\n",
    "- Websites often **restrict access to bots or automated tools like Selenium**\n",
    "- The website might have anti-scraping mechanisms in place\n",
    "- The certificate or SSL/TLS version might not be supported or configured correctly\n",
    "#### Solutions:\n",
    "- Ensure your WebDriver matches the browser version\n",
    "- Configure browser **options in Selenium**\n",
    "- Force the browser to use HTTP/1.1\n",
    "- Use a **user-agent string to make requests** appear like they are coming from a browser\n",
    "- Run the browser in **incognito mode**\n",
    "## Selenium Options:\n",
    "- In Selenium, **Options** is used to **customize and configure browser settings and behavior** when automating browsers\n",
    "- Allows to specify preferences, enable or disable features, and set options that are specific to the browser you are automating\n",
    "- Each browser driver has its own Options class to provide these configurations\n",
    "#### Common Usecases:\n",
    "- Running browser in headless mode (without GUI)\n",
    "- Disabling browser notifications\n",
    "- Setting the default download directory\n",
    "- Adding browser extensions\n",
    "#### Key Methods:\n",
    "- **add_argument(arg)**: **Adds a command-line argument** to the browser\n",
    "- **add_experimental_option(name, value)**: Adds experimental options or preferences\n",
    "- **set_capability(name, value)**: Sets desired capabilities for the browser\n",
    "#### Advantages:\n",
    "- Helps customize browser behavior according to your testing needs\n",
    "- **Suppress unnecessary UI elements like notifications or pop-ups**\n",
    "- Ensure the browser runs with specific settings each time, promoting consistency\n",
    "- Improves efficiency as we can run browsers in **headless mode to save resources** during testing\n",
    "## getBoundingClientRect():\n",
    "- JavaScript function, used to manipulate and interact with elements on a webpage by executing JavaScript\n",
    "- useful when standard Selenium methods can't achieve certain tasks directly, such as **precise scrolling or positioning**\n",
    "- This approach ensures that Selenium scripts can interact with dynamically loaded or partially visible elements more reliably\n",
    "#### Advantages:\n",
    "- **Helps in cases where the elements are not immediately visible in the viewport**\n",
    "- Handles scenarios where the webpage layout changes, requiring precise adjustments\n",
    "- Ensures elements are well-placed for user interaction or screenshots\n",
    "- Works even when the standard Selenium methods (**scrollIntoView()**, **actions.move_to_element()**, etc.) don't work as intended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b1a8f0-3fa9-4369-b7f4-9ac1e5a428b7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2440685-5f37-4ff9-afc7-da5172befb74",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7745393-3f9e-4704-af21-da7158fa1d44",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscraping",
   "language": "python",
   "name": "webscraping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
